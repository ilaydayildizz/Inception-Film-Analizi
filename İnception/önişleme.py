import pandas as pd
import re
import nltk
import emoji
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# ===============================
# NLTK SETUP
# ===============================
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Film iÃ§in anlamsal katkÄ±sÄ± dÃ¼ÅŸÃ¼k kelimeler
domain_stopwords = {"movie", "film"}

def on_isleme(metin):
    if pd.isna(metin) or len(str(metin).strip()) < 2:
        return ""

    metin = str(metin)

    # 1ï¸âƒ£ Emoji temizleme
    metin = emoji.replace_emoji(metin, replace=" ")

    # 2ï¸âƒ£ URL / mention / hashtag temizleme
    metin = re.sub(r"http\S+|www\.\S+", " ", metin)
    metin = re.sub(r"[@#]\w+", " ", metin)

    # 3ï¸âƒ£ KÃ¼Ã§Ã¼k harf
    metin = metin.lower()

    # 4ï¸âƒ£ Negatif yapÄ±larÄ± koru (Ã‡OK Ã–NEMLÄ°)
    metin = re.sub(r'\bnot (\w+)\b', r'not_\1', metin)

    # 5ï¸âƒ£ Noktalama & sayÄ± temizleme
    metin = re.sub(r'[^\w\s]', ' ', metin)
    metin = re.sub(r'\d+', ' ', metin)

    # 6ï¸âƒ£ ASCII dÄ±ÅŸÄ± karakterleri sil
    metin = metin.encode("utf-8", "ignore").decode("ascii", "ignore")

    # 7ï¸âƒ£ Stopword + Lemmatization
    kelimeler = metin.split()
    temiz_kelimeler = [
        lemmatizer.lemmatize(k)
        for k in kelimeler
        if k not in stop_words and k not in domain_stopwords
    ]

    metin = " ".join(temiz_kelimeler)

    # 8ï¸âƒ£ BoÅŸluk dÃ¼zenleme
    metin = re.sub(r'\s+', ' ', metin).strip()

    return metin


# ===============================
# ANA Ä°ÅžLEM
# ===============================
print("ðŸ“¥ Excel yÃ¼kleniyor...")
df = pd.read_excel("tum_yorumlar_translated_en.xlsx")

print("ðŸ§¹ Ã–n iÅŸleme uygulanÄ±yor...")
df["temiz_yorum"] = df["yorum_english"].apply(on_isleme)

# Ã‡ok kÄ±sa / anlamsÄ±z yorumlarÄ± sil
df = df[df["temiz_yorum"].str.split().str.len() > 1]

print("ðŸ’¾ Kaydediliyor...")
df.to_excel("temizlenmis_yorumlar_final.xlsx", index=False)

print("ðŸŽ‰ Bitti! Analize %100 hazÄ±r.")
